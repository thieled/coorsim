% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/05_generate_community_labels.R
\name{label_users}
\alias{label_users}
\title{Label Users Using a Local LLM}
\usage{
label_users(
  groups_data,
  model = "llama3.2:3b",
  prompt = NULL,
  system = NULL,
  example = NULL,
  answer = NULL,
  schema = NULL,
  retries = 3,
  retry_trunc = 4000,
  seed = 42,
  temp = 0,
  verbose = TRUE
)
}
\arguments{
\item{groups_data}{A list containing at least the element \code{user_labels}, 
typically produced by [sample_user_text()]. This element must include a column \code{text} 
with sampled user content and an \code{account_id} column.}

\item{model}{Character string. Name of the local LLM model to be queried via Ollama. 
Default is \code{"llama3.2:3b"}.}

\item{prompt}{Optional character string providing user-facing task instructions 
(defaults to the internal \code{prompts$prompt_user}).}

\item{system}{Optional character string defining the system-level instruction to guide model behavior 
(defaults to the internal \code{prompts$system_user}).}

\item{example}{Optional character vector containing example input texts 
(defaults to \code{examples$example_user_text}).}

\item{answer}{Optional character vector containing corresponding example model outputs 
(defaults to \code{examples$example_user_answer}).}

\item{schema}{Optional JSON-like list defining the expected output structure 
(defaults to \code{schemata$schema_user}).}

\item{retries}{Integer. Maximum number of re-query attempts for responses that fail JSON validation. 
Default is \code{3}.}

\item{retry_trunc}{Integer. Character limit for truncating user text during retry attempts to prevent 
context overflow. Default is \code{4000}.}

\item{seed}{Integer. Random seed used for model sampling; incremented by 1 on each retry. Default is \code{42}.}

\item{temp}{Numeric. Sampling temperature for model generation (0 = deterministic). Default is \code{0.0}.}

\item{verbose}{Logical. Whether to display progress messages and retry information. Default is \code{TRUE}.}
}
\value{
The input \code{groups_data} list with the \code{user_labels} element updated to include:
\describe{
  \item{\code{description}}{Concise summary generated by the model.}
  \item{\code{lang}}{Predicted language code (ISO format).}
  \item{\code{emotion_valence}}{Overall emotional tone ("positive", "negative", "neutral").}
  \item{\code{incivility}}{Presence of uncivil or offensive language ("yes", "no").}
  \item{\code{elaborate}}{Linguistic elaboration category ("simple", "moderate", or "elaborate").}
  \item{\code{confidence}}{Model-assessed confidence (0–1 scale).}
  \item{\code{topic_1}–\code{topic_5}}{Detected main topics.}
  \item{\code{pattern_1}–\code{pattern_3}}{Repeated emojis, slogans, or stylistic markers.}
  \item{\code{named_entity_1}–\code{named_entity_3}, \code{sentiment_1}–\code{sentiment_3}}{Named entities 
        and their corresponding sentiment labels.}
  \item{\code{is_valid_json}}{Logical flag indicating valid JSON structure in model output.}
  \item{\code{response}}{Raw model output (JSON string).}
  \item{\code{model}, \code{model_queried_at}, \code{model_total_duration}}{Metadata for reproducibility.}
}
}
\description{
This function uses a locally hosted large language model (LLM) through the 
\pkg{rollama} interface to automatically generate structured annotations for users 
based on their sampled posts. It applies a few-shot prompting setup with examples 
and a predefined schema to produce machine-readable JSON output describing each user.
}
\details{
The model should run via an active \emph{Ollama} Docker container with GPU (CUDA) support.
Use `system("docker ps")` to verify that Ollama is running, and ensure that the 
desired model (e.g., `"llama3.2:3b"`) has been pulled via Ollama before executing this function.


The function constructs few-shot prompts by combining examples and instructions, sends them to the 
locally hosted model using [rollama::query()], and validates the returned JSON using \pkg{jsonlite}.
If a response fails validation, it retries up to \code{retries} times with shorter input text 
(\code{retry_trunc}) and incremented seed values.

The schema guides the model to output structured JSON containing linguistic, emotional, and stylistic annotations 
for each user. Common fields include `description`, `lang`, `topic`, `named_entities`, `repetitive_patterns`, 
`emotion_valence`, `incivility`, `elaborate`, and `confidence`.

Ollama must be accessible locally (e.g., via Docker). Use [rollama::ping_ollama()] to confirm connectivity.
}
\seealso{
[sample_user_text()], [rollama::query()], [rollama::make_query()], 
[jsonlite::fromJSON()], [data.table::as.data.table()]
}
